## Tokenizer
A language model places a probability distribution over sequences of tokens (usually represented by integer indices). So we need a procedure that encodes strings into tokens. Also a procedure decodes tokens back to string.
## Unicode
Unicode is a text encoding standard that maps characters to integer code points. For example, the character “s” has the code point 115 (typically notated as U+0073, where U+ is a conventional prefix and 0073 is 115 in hexadecimal), and the character “牛” has the code point 29275.

### why not train tokenizer directly on unicode (character-based tokenizer)
- Unicode has 154,998 characters, 155k vocabulary size. Too **large**. GPT-2 has around 50K
- **sparse**, since many characters are quite rare, which makes model hard to learn reliably

Instead, we use **UTF-8**, the dominant character encoding for the web, representing any Unicode character (letters, emojis, symbols) using 1 to 4 bytes

By converting our Unicode codepoints into a sequence of bytes (e.g., via the UTF-8 encoding), we are essentially taking a sequence of codepoints (integers in the range 0 to 154,997) and transforming it into a sequence of byte values (integers in the range 0 to 255). 256-length byte vocabulary is much more manageable.

### Other Tokenizer
- **word tokenizer**: Vocabulary size is # of distinct segments in the training data. New words haven't seen during training will experience out-of-vocabulary and get \<unk\> token
- **byte-level tokenizer**: Vocabulary size is 256, compression ratio = # token / # bytes = 1. Compression ratio stands for how many bytes does a token cover on average. Compression ratio low means, longger sequence length, more computation in attention layer.

## Subword Tokenizer(Byte-pair tokenizer)
midpoint between word-level and byte-level tokenizer.

### Special token
some strings as special token should never split into multiple tokens, such as \<|endoftext|> which is represent boundaries between document.

### Algorithm
- Initialize vocabulary
- pre-tokenization
- Merge

